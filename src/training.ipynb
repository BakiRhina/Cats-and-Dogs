{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries (torch, matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as opt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/Ato/Documents/Programming/Python/catdog/src/datasets\"\n",
    "dataset_path = PATH + \"/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entries and number of samples of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(dataset_path)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Taking a look at the filenames, it is possible extracting the name and label with ease with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(dataset_path)\n",
    "labels = [filename.split('.')[0] for filename in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'filename': filenames, 'label': labels}\n",
    "df = pd.DataFrame(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat.0.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat.1.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat.10.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat.100.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat.1000.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename label\n",
       "0     cat.0.jpg   cat\n",
       "1     cat.1.jpg   cat\n",
       "2    cat.10.jpg   cat\n",
       "3   cat.100.jpg   cat\n",
       "4  cat.1000.jpg   cat"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>25000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>cat.0.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  label\n",
       "count       25000  25000\n",
       "unique      25000      2\n",
       "top     cat.0.jpg    cat\n",
       "freq            1  12500"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bar_labels = df['label'].value_counts().index\n",
    "counts = df['label'].value_counts().values\n",
    "bar_colors = ['tab:red', 'tab:blue']\n",
    "\n",
    "ax.bar(bar_labels, counts, color=bar_colors)\n",
    "ax.legend(title=\"Data balance check\")\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is balanced! Let's continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizing dimensions, shapes, sizes,...\n",
    "\n",
    "- Take a smaller sample but big enough to contain useful information. sample = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 5000\n",
    "sample_images = df['filename'].sample(sample_size).to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = []\n",
    "height = []\n",
    "channels = [] # channels\n",
    "\n",
    "for filename in sample_images:\n",
    "  image = cv2.imread(os.path.join(dataset_path, filename), 1)\n",
    "  width.append(image.shape[1])\n",
    "  height.append(image.shape[0])\n",
    "  channels.append(image.shape[2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>channels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>412</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>499</td>\n",
       "      <td>375</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>499</td>\n",
       "      <td>340</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>319</td>\n",
       "      <td>215</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   width  height  channels\n",
       "0    412     500         3\n",
       "1    499     375         3\n",
       "2    499     340         3\n",
       "3    324     242         3\n",
       "4    319     215         3"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dimensions = pd.DataFrame({\n",
    "  'width':width,\n",
    "  'height':height,  \n",
    "  'channels':channels\n",
    "})\n",
    "\n",
    "sample_dimensions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dimensions['aspect ratio'] = sample_dimensions['width']/sample_dimensions['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>channels</th>\n",
       "      <th>aspect ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>412</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>499</td>\n",
       "      <td>375</td>\n",
       "      <td>3</td>\n",
       "      <td>1.330667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>499</td>\n",
       "      <td>340</td>\n",
       "      <td>3</td>\n",
       "      <td>1.467647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>1.338843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>319</td>\n",
       "      <td>215</td>\n",
       "      <td>3</td>\n",
       "      <td>1.483721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   width  height  channels  aspect ratio\n",
       "0    412     500         3      0.824000\n",
       "1    499     375         3      1.330667\n",
       "2    499     340         3      1.467647\n",
       "3    324     242         3      1.338843\n",
       "4    319     215         3      1.483721"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dimensions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>channels</th>\n",
       "      <th>aspect ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>407.502000</td>\n",
       "      <td>363.84700</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.155639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>107.323446</td>\n",
       "      <td>96.15383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.375940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>335.000000</td>\n",
       "      <td>312.00000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.924922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>450.000000</td>\n",
       "      <td>374.00000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.275994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>427.00000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.336898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.994012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             width      height  channels  aspect ratio\n",
       "count  5000.000000  5000.00000    5000.0   5000.000000\n",
       "mean    407.502000   363.84700       3.0      1.155639\n",
       "std     107.323446    96.15383       0.0      0.286686\n",
       "min      50.000000    38.00000       3.0      0.375940\n",
       "25%     335.000000   312.00000       3.0      0.924922\n",
       "50%     450.000000   374.00000       3.0      1.275994\n",
       "75%     499.000000   427.00000       3.0      1.336898\n",
       "max     500.000000   500.00000       3.0      2.994012"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dimensions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the percentile 95th, so we know what value the majority of the features have.\n",
    "\n",
    "It seems that most of the values are below 500 in width and height and hte aspect ratio is about 1.5, which means that the width is usually 1.5 times bigger than the height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 500.0, 1.5063048088496591)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(width, 0.95), np.quantile(height, 0.95), np.quantile(sample_dimensions['aspect ratio'], 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trying with different methods to obtain the quantile. All produce the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 500, 500, 500.0, 500.0, 500.0)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(width, 0.95, method='hazen'), np.quantile(width, 0.95, method='closest_observation'), np.quantile(width, 0.95, method='higher'), np.quantile(width, 0.95, method='averaged_inverted_cdf'), np.quantile(width, 0.95, method='linear'), np.quantile(width, 0.95, method='median_unbiased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesandnames = sample_dimensions\n",
    "#print(sample_images[:10])\n",
    "samplesandnames['filename'] = sample_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum goal is to preserve quality. From the quantiles, it has been deducted that most of the width and height values fall below 500 while the aspect ratio (AR) is 1.5 in the majority of cases. More data is lost when oversampling the images than when undersampling them, so only the smallest widths and heghts will be removed. Since most of the aspect ratio values fall below 1.5, all AR values between 0.7 and 2 will be removed as well.\n",
    "\n",
    "- Width > 150\n",
    "- Height > 100\n",
    "- 0.7 > Aspect Ratio > 2\n",
    "\n",
    "These values can be tuned to obtain different results.\n",
    "**The smaller the range, the lesser data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_samples = samplesandnames[\n",
    "  (samplesandnames['width'] >= 150) &\n",
    "  (samplesandnames['height'] >= 100) &\n",
    "  (samplesandnames['aspect ratio'] >= 0.7) &\n",
    "  (samplesandnames['aspect ratio'] <= 2)\n",
    "]\n",
    "\n",
    "processed_samples = processed_samples.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>channels</th>\n",
       "      <th>aspect ratio</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>412</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>cat.3027.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>499</td>\n",
       "      <td>375</td>\n",
       "      <td>3</td>\n",
       "      <td>1.330667</td>\n",
       "      <td>cat.10302.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>499</td>\n",
       "      <td>340</td>\n",
       "      <td>3</td>\n",
       "      <td>1.467647</td>\n",
       "      <td>dog.10900.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>1.338843</td>\n",
       "      <td>dog.11716.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>319</td>\n",
       "      <td>215</td>\n",
       "      <td>3</td>\n",
       "      <td>1.483721</td>\n",
       "      <td>cat.967.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   width  height  channels  aspect ratio       filename\n",
       "0    412     500         3      0.824000   cat.3027.jpg\n",
       "1    499     375         3      1.330667  cat.10302.jpg\n",
       "2    499     340         3      1.467647  dog.10900.jpg\n",
       "3    324     242         3      1.338843  dog.11716.jpg\n",
       "4    319     215         3      1.483721    cat.967.jpg"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the **outliers** have been deleted, the amount of data reduced needs to be checked out. Looking at the outcome, most of the images are not considered outliers, although almost 10% of the sample data has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>channels</th>\n",
       "      <th>aspect ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4580.000000</td>\n",
       "      <td>4580.000000</td>\n",
       "      <td>4580.0</td>\n",
       "      <td>4580.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>422.016376</td>\n",
       "      <td>365.265721</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.184853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>95.570114</td>\n",
       "      <td>87.720510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>357.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.987495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>479.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.324572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.336898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.996000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             width       height  channels  aspect ratio\n",
       "count  4580.000000  4580.000000    4580.0   4580.000000\n",
       "mean    422.016376   365.265721       3.0      1.184853\n",
       "std      95.570114    87.720510       0.0      0.247496\n",
       "min     150.000000   104.000000       3.0      0.700000\n",
       "25%     357.000000   320.000000       3.0      0.987495\n",
       "50%     479.000000   374.000000       3.0      1.324572\n",
       "75%     499.000000   413.000000       3.0      1.336898\n",
       "max     500.000000   500.000000       3.0      1.996000"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_samples.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Preprocessing\n",
    "\n",
    "- The same sample preprocessing is now applied to all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       filename label\n",
       " 0     cat.0.jpg   cat\n",
       " 1     cat.1.jpg   cat\n",
       " 2    cat.10.jpg   cat\n",
       " 3   cat.100.jpg   cat\n",
       " 4  cat.1000.jpg   cat,\n",
       " 25000)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(), len(df.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before removing outliers, 10% of the data (len(df)*0.1) that will be used for testing the model is substracted from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = df.sample(n=int(len(df)*0.1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding ALL images shapes to the lists. Later the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = []\n",
    "height = []\n",
    "\n",
    "for filename in df['filename']:\n",
    "  image = cv2.imread(os.path.join(dataset_path, filename))  \n",
    "  width.append(image.shape[1])\n",
    "  height.append(image.shape[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Note\n",
    "\n",
    "- Width > 100\n",
    "- Height > 150\n",
    "- 0.7 > Aspect Ratio > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.DataFrame({'filename': df['filename'], 'width': width, 'height': height, 'label': df['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(             width        height\n",
       " count  25000.00000  25000.000000\n",
       " mean     404.09904    360.478080\n",
       " std      109.03793     97.019959\n",
       " min       42.00000     32.000000\n",
       " 25%      323.00000    301.000000\n",
       " 50%      447.00000    374.000000\n",
       " 75%      499.00000    421.000000\n",
       " max     1050.00000    768.000000,\n",
       "          filename  label\n",
       " count       25000  25000\n",
       " unique      25000      2\n",
       " top     cat.0.jpg    cat\n",
       " freq            1  12500)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.describe(), df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['aspect ratio'] = full_data['width'] / full_data['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a mask to identify the filenames in both full_data and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     25000\n",
       "unique        2\n",
       "top       False\n",
       "freq      22500\n",
       "Name: filename, dtype: object"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = full_data['filename'].isin(testing['filename'])\n",
    "mask.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>aspect ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22500.000000</td>\n",
       "      <td>22500.000000</td>\n",
       "      <td>22500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>404.154889</td>\n",
       "      <td>360.720222</td>\n",
       "      <td>1.156533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>108.965233</td>\n",
       "      <td>96.990081</td>\n",
       "      <td>0.290620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.306613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>323.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>0.929455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>1.268116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>1.336898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1050.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>3.815789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              width        height  aspect ratio\n",
       "count  22500.000000  22500.000000  22500.000000\n",
       "mean     404.154889    360.720222      1.156533\n",
       "std      108.965233     96.990081      0.290620\n",
       "min       42.000000     32.000000      0.306613\n",
       "25%      323.000000    301.000000      0.929455\n",
       "50%      447.000000    374.000000      1.268116\n",
       "75%      499.000000    421.000000      1.336898\n",
       "max     1050.000000    768.000000      3.815789"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = full_data[~mask]\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[\n",
    "  (train_data['width'] >= 100) &\n",
    "  (train_data['height'] >= 150) &\n",
    "  (train_data['aspect ratio'] >= 0.7) &\n",
    "  (train_data['aspect ratio'] <= 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking if the data is still balanced after preprocessing. It is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['label'].value_counts().sort_index().plot(kind='bar', color=['blue', 'red'])\n",
    "\n",
    "plt.title('Filtered dataset Balance')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle\n",
    "\n",
    "Before splitting into training and validation it is necessary to shuffle the data to avoid overfitting due to predetermined orders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>aspect ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20306.000000</td>\n",
       "      <td>20306.000000</td>\n",
       "      <td>20306.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>421.315276</td>\n",
       "      <td>363.581995</td>\n",
       "      <td>1.185331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>95.962193</td>\n",
       "      <td>86.050366</td>\n",
       "      <td>0.247576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>109.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>352.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>0.989467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>479.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>1.323607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>1.336898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1050.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              width        height  aspect ratio\n",
       "count  20306.000000  20306.000000  20306.000000\n",
       "mean     421.315276    363.581995      1.185331\n",
       "std       95.962193     86.050366      0.247576\n",
       "min      109.000000    150.000000      0.700000\n",
       "25%      352.000000    315.000000      0.989467\n",
       "50%      479.000000    374.000000      1.323607\n",
       "75%      499.000000    405.000000      1.336898\n",
       "max     1050.000000    768.000000      2.000000"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frac = 1 -> all data\n",
    "# drop=True --> removes old index and creates a new one for the new sorted dataframe\n",
    "# If no index is desired:\n",
    "#   1. full_data.sample(frac=1).reset_index(drop=True, inplace=True) -> with the original dataframe (full_data)\n",
    "#   2. shuffled_data.to_csv('shuffled_data.csv', index=False) -> with a new variable\n",
    "shuffled_data = train_data.sample(frac=1).reset_index(drop=True) \n",
    "shuffled_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation - Training set split\n",
    "\n",
    "Since the features width, height and aspect ratio were created only for preprocessing purposes it is safe to remove them. Furthermore, it is a good practice to create a checkpoint of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label</th>\n",
       "      <th>aspect ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog.9417.jpg</td>\n",
       "      <td>500</td>\n",
       "      <td>363</td>\n",
       "      <td>dog</td>\n",
       "      <td>1.377410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat.242.jpg</td>\n",
       "      <td>500</td>\n",
       "      <td>374</td>\n",
       "      <td>cat</td>\n",
       "      <td>1.336898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog.4811.jpg</td>\n",
       "      <td>499</td>\n",
       "      <td>412</td>\n",
       "      <td>dog</td>\n",
       "      <td>1.211165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat.8740.jpg</td>\n",
       "      <td>500</td>\n",
       "      <td>374</td>\n",
       "      <td>cat</td>\n",
       "      <td>1.336898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat.11097.jpg</td>\n",
       "      <td>500</td>\n",
       "      <td>374</td>\n",
       "      <td>cat</td>\n",
       "      <td>1.336898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  width  height label  aspect ratio\n",
       "0   dog.9417.jpg    500     363   dog      1.377410\n",
       "1    cat.242.jpg    500     374   cat      1.336898\n",
       "2   dog.4811.jpg    499     412   dog      1.211165\n",
       "3   cat.8740.jpg    500     374   cat      1.336898\n",
       "4  cat.11097.jpg    500     374   cat      1.336898"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint\n",
    "\n",
    "'''\n",
    "Internally, in python, copy() method uses a pointer to the original data, which\n",
    "means that changes to the new object will affect the old. To overcome this, set deep=True.\n",
    "For non-pandas objects, use copy.deepcopy(<object>)\n",
    "'''\n",
    "\n",
    "data = shuffled_data.copy(deep=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog.9417.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat.242.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog.4811.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat.8740.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat.11097.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename label\n",
       "0   dog.9417.jpg   dog\n",
       "1    cat.242.jpg   cat\n",
       "2   dog.4811.jpg   dog\n",
       "3   cat.8740.jpg   cat\n",
       "4  cat.11097.jpg   cat"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unwwanted features\n",
    "\n",
    "data = data.drop(['width', 'height', 'aspect ratio'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set integers for labels instead of strings using a simple lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat.9164.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat.8804.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat.7257.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog.1903.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat.2065.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename label\n",
       "0  cat.9164.jpg   cat\n",
       "1  cat.8804.jpg   cat\n",
       "2  cat.7257.jpg   cat\n",
       "3  dog.1903.jpg   dog\n",
       "4  cat.2065.jpg   cat"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        filename  label\n",
       " 0   dog.9417.jpg      1\n",
       " 1    cat.242.jpg      0\n",
       " 2   dog.4811.jpg      1\n",
       " 3   cat.8740.jpg      0\n",
       " 4  cat.11097.jpg      0,\n",
       "        filename  label\n",
       " 0  cat.9164.jpg      0\n",
       " 1  cat.8804.jpg      0\n",
       " 2  cat.7257.jpg      0\n",
       " 3  dog.1903.jpg      1\n",
       " 4  cat.2065.jpg      0)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"] = data[\"label\"].apply(lambda x: 1 if x == 'dog' else 0)\n",
    "testing[\"label\"] = testing[\"label\"].apply(lambda x: 1 if x == 'dog' else 0)\n",
    "data.head(), testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20306"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_length = len(data.index)\n",
    "data_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, there are still more than 20000 images. A ratio of 80% (training) 20% (validation) for this amoount of data seems correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded 16245\n",
      "int 16244\n",
      "float 16244.800000000001\n",
      "Error round: 0.1999999999989086\n",
      "Error int: 0.8000000000010914\n"
     ]
    }
   ],
   "source": [
    "train_len_round = round(len(data.index)*0.8)\n",
    "train_len_int = int(len(data.index)*0.8)\n",
    "train_len_float = len(data.index)*0.8\n",
    "\n",
    "print(f'Rounded {train_len_round}')\n",
    "print(f'int {train_len_int}')\n",
    "print(f'float {train_len_float}')\n",
    "\n",
    "print(f'Error round: {abs(train_len_float-train_len_round)}')\n",
    "print(f'Error int: {abs(train_len_float-train_len_int)}')\n",
    "\n",
    "# More precision using round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16245, 2), (4061, 2))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = data.iloc[:train_len_round,:]\n",
    "val_df = data.iloc[train_len_round:, :]\n",
    "\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8000098493056239, 0.19999015069437603)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape[0]/data_length, val_df.shape[0]/data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolut errors: [9.849305623865767e-06, 9.84930562397679e-06]\n"
     ]
    }
   ],
   "source": [
    "abs_err_train = abs(0.8-train_df.shape[0]/data_length)\n",
    "abs_err_val = abs(0.2-val_df.shape[0]/data_length)\n",
    "\n",
    "errors = [abs_err_train, abs_err_val]\n",
    "\n",
    "print(f'Absolut errors: {errors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting completed with a small error (Ea ~ 9.849e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Pytorch\n",
    "\n",
    "The dataset will be fed to a convolutional neural network model. This model is defined below and its main file can be found in `/modules/cnn.py`. For the moment, it has 2 convolutional layers, 2 pooling layers and 3 fully connected layers. The input depends on the size of the kernels and the image resizing.\n",
    "To calculate the input-size in the neural networks, refer to https://github.com/BakiRhina/Cats-and-Dogs#important-note-on-fc1-shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3,6,5) # (N_in_channels, N_out_channels, kernel_size)\n",
    "    self.conv2 = nn.Conv2d(6,16,5)\n",
    "    self.pool = nn.MaxPool2d(2,2)\n",
    "    self.fc1 = nn.Linear(16*93*61, 256)\n",
    "    self.fc2 = nn.Linear(256, 120)\n",
    "    self.fc3 = nn.Linear(120, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = torch.flatten(x, 1) \n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks, specially if fed with big datasets containing images, consume lots of resources. That is why using cudas gpu is a good option if available, to reduce time consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Select GPU if available (cuda toolkit <= 11.8 for pytorch)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Enables benchmark mode in cudnn to find the best algorithm to use for your hardware.\n",
    "#more info --> https://pytorch.org/docs/stable/backends.html#torch.backends.mps.is_built\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, looking at the 95% percentile, the aspect ratio is around 1.5. Therefore, some combinations to try out, without loosing much resolution and information would be: (384,256,3);(256,256,3);(128,128,3);..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to try different sizes\n",
    "\n",
    "input_shape = (384,256)\n",
    "#input_shape = (256,256)\n",
    "#input_shape = (128,128)\n",
    "\n",
    "df_transform = transforms.Compose([\n",
    "  transforms.Resize(input_shape),\n",
    "  transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16245.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.496584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  16245.000000\n",
       "mean       0.496584\n",
       "std        0.500004\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why the custom dataset? It is possible to use the class Dataset to create Dataset-like objects and then pass it to the DataLoader. This class is optimized to be less memory consuming (processes data in separate cores in real time and feed it immediately to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataframe, root_dir , transform=None):\n",
    "    self.dataframe = dataframe\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "  \n",
    "  #DataLoader will automatically use these methods (__len__() and __getitem__())\n",
    "  #To calculate the amount of iterations with the selected batch\n",
    "  def __len__(self):\n",
    "    return len(self.dataframe)\n",
    "\n",
    "  #To select the samples of data form the dataframe\n",
    "  def __getitem__(self, idx):\n",
    "    img_filepath = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n",
    "    image = Image.open(img_filepath).convert(\"RGB\")\n",
    "    label = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "\n",
    "    return image, label\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instance of training and validating sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_df, dataset_path, transform=df_transform)\n",
    "val_set = CustomDataset(val_df, dataset_path, transform=df_transform)\n",
    "testing_set = CustomDataset(testing, dataset_path, transform=df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Torch loader with batch size of 16/32, and shuffled again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 0.0001\n",
    "momentum = 0.9\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "testing_loader = DataLoader(testing_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "[1,    50] loss: 0.01391\n",
      "[1,   100] loss: 0.01378\n",
      "[1,   150] loss: 0.01396\n",
      "[1,   200] loss: 0.01382\n",
      "[1,   250] loss: 0.01388\n",
      "[1,   300] loss: 0.01387\n",
      "[1,   350] loss: 0.01383\n",
      "[1,   400] loss: 0.01384\n",
      "[1,   450] loss: 0.01387\n",
      "[1,   500] loss: 0.01385\n",
      "Epoch 1\n",
      "[2,    50] loss: 0.01384\n",
      "[2,   100] loss: 0.01384\n",
      "[2,   150] loss: 0.01384\n",
      "[2,   200] loss: 0.01379\n",
      "[2,   250] loss: 0.01383\n",
      "[2,   300] loss: 0.01376\n",
      "[2,   350] loss: 0.01379\n",
      "[2,   400] loss: 0.01385\n",
      "[2,   450] loss: 0.01385\n",
      "[2,   500] loss: 0.01386\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = opt.SGD(cnn.parameters(), lr, momentum)\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "  print(f'Epoch {epoch}')\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    imgs, label = data\n",
    "    imgs, label = imgs.to(device), label.to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass + Backward pass + Gradient optimization\n",
    "    output = cnn(imgs)\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    if i % 50 == 49:\n",
    "      running_loss += loss.item()\n",
    "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.5f}')\n",
    "      loss_values.append(running_loss)\n",
    "      running_loss = 0.0\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'model11.pth'\n",
    "model_path = f'models/{model_filename}'\n",
    "\n",
    "torch.save(cnn.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FC1 input calculation\n",
    "\n",
    "When changing the structure of the CNN or the batch size, run this to know the dimensions of the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 384, 256])\n",
      "torch.Size([32, 6, 380, 252])\n",
      "torch.Size([32, 6, 190, 126])\n",
      "torch.Size([32, 16, 186, 122])\n",
      "torch.Size([32, 16, 93, 61])\n",
      "torch.Size([32, 90768])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(3,6,5)\n",
    "pool = nn.MaxPool2d(2,2)\n",
    "conv2 = nn.Conv2d(6,16,5)\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "x = conv1(images)\n",
    "print(x.shape)\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "x = conv2(x)\n",
    "print(x.shape)\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "x = torch.flatten(x, 1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of CNN(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=90768, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=120, bias=True)\n",
       "  (fc3): Linear(in_features=120, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "cnn.load_state_dict(torch.load(model_path))\n",
    "cnn.eval()\n",
    "cnn.to(device)\n",
    "cnn.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "tensor([[-0.0544, -0.0456],\n",
      "        [-0.0349, -0.0679],\n",
      "        [-0.0531, -0.0462],\n",
      "        [-0.0565, -0.0400],\n",
      "        [-0.0556, -0.0406],\n",
      "        [-0.0421, -0.0621],\n",
      "        [-0.0448, -0.0601],\n",
      "        [-0.0478, -0.0581],\n",
      "        [-0.0428, -0.0600],\n",
      "        [-0.0603, -0.0369],\n",
      "        [-0.0413, -0.0582],\n",
      "        [-0.0388, -0.0671],\n",
      "        [-0.0497, -0.0502],\n",
      "        [-0.0423, -0.0585],\n",
      "        [-0.0438, -0.0632],\n",
      "        [-0.0537, -0.0519],\n",
      "        [-0.0354, -0.0674],\n",
      "        [-0.0480, -0.0533],\n",
      "        [-0.0447, -0.0622],\n",
      "        [-0.0442, -0.0589],\n",
      "        [-0.0685, -0.0268],\n",
      "        [-0.0333, -0.0723],\n",
      "        [-0.0364, -0.0646],\n",
      "        [-0.0407, -0.0618],\n",
      "        [-0.0548, -0.0479],\n",
      "        [-0.0615, -0.0400],\n",
      "        [-0.0336, -0.0699],\n",
      "        [-0.0379, -0.0672],\n",
      "        [-0.0486, -0.0537],\n",
      "        [-0.0461, -0.0551],\n",
      "        [-0.0554, -0.0468],\n",
      "        [-0.0344, -0.0675]], device='cuda:0')\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([[-0.0517, -0.0530],\n",
      "        [-0.0474, -0.0534],\n",
      "        [-0.0617, -0.0405],\n",
      "        [-0.0579, -0.0438],\n",
      "        [-0.0420, -0.0597],\n",
      "        [-0.0541, -0.0453],\n",
      "        [-0.0537, -0.0449],\n",
      "        [-0.0322, -0.0693],\n",
      "        [-0.0344, -0.0679],\n",
      "        [-0.0555, -0.0412],\n",
      "        [-0.0430, -0.0607],\n",
      "        [-0.0414, -0.0611],\n",
      "        [-0.0662, -0.0354],\n",
      "        [-0.0568, -0.0434],\n",
      "        [-0.0482, -0.0586],\n",
      "        [-0.0282, -0.0751],\n",
      "        [-0.0352, -0.0659],\n",
      "        [-0.0567, -0.0443],\n",
      "        [-0.0371, -0.0663],\n",
      "        [-0.0523, -0.0506],\n",
      "        [-0.0467, -0.0544],\n",
      "        [-0.0378, -0.0655],\n",
      "        [-0.0553, -0.0460],\n",
      "        [-0.0386, -0.0625],\n",
      "        [-0.0520, -0.0519],\n",
      "        [-0.0494, -0.0530],\n",
      "        [-0.0336, -0.0699],\n",
      "        [-0.0541, -0.0468],\n",
      "        [-0.0417, -0.0579],\n",
      "        [-0.0373, -0.0641],\n",
      "        [-0.0505, -0.0479],\n",
      "        [-0.0437, -0.0559]], device='cuda:0')\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0], device='cuda:0')\n",
      "tensor([[-0.0554, -0.0437],\n",
      "        [-0.0561, -0.0435],\n",
      "        [-0.0338, -0.0722],\n",
      "        [-0.0589, -0.0397],\n",
      "        [-0.0514, -0.0412],\n",
      "        [-0.0494, -0.0518],\n",
      "        [-0.0473, -0.0546],\n",
      "        [-0.0535, -0.0473],\n",
      "        [-0.0539, -0.0483],\n",
      "        [-0.0456, -0.0570],\n",
      "        [-0.0295, -0.0756],\n",
      "        [-0.0539, -0.0475],\n",
      "        [-0.0392, -0.0603],\n",
      "        [-0.0495, -0.0505],\n",
      "        [-0.0455, -0.0566],\n",
      "        [-0.0406, -0.0617],\n",
      "        [-0.0422, -0.0636],\n",
      "        [-0.0375, -0.0637],\n",
      "        [-0.0309, -0.0735],\n",
      "        [-0.0641, -0.0357],\n",
      "        [-0.0420, -0.0579],\n",
      "        [-0.0590, -0.0453],\n",
      "        [-0.0472, -0.0544],\n",
      "        [-0.0494, -0.0536],\n",
      "        [-0.0443, -0.0546],\n",
      "        [-0.0400, -0.0608],\n",
      "        [-0.0483, -0.0549],\n",
      "        [-0.0311, -0.0703],\n",
      "        [-0.0406, -0.0594],\n",
      "        [-0.0318, -0.0700],\n",
      "        [-0.0478, -0.0569],\n",
      "        [-0.0473, -0.0545]], device='cuda:0')\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[-0.0460, -0.0549],\n",
      "        [-0.0653, -0.0306],\n",
      "        [-0.0334, -0.0727],\n",
      "        [-0.0588, -0.0356],\n",
      "        [-0.0505, -0.0500],\n",
      "        [-0.0518, -0.0512],\n",
      "        [-0.0340, -0.0715],\n",
      "        [-0.0339, -0.0715],\n",
      "        [-0.0455, -0.0568],\n",
      "        [-0.0495, -0.0526],\n",
      "        [-0.0540, -0.0468],\n",
      "        [-0.0514, -0.0500],\n",
      "        [-0.0520, -0.0523],\n",
      "        [-0.0433, -0.0575],\n",
      "        [-0.0330, -0.0677],\n",
      "        [-0.0423, -0.0642],\n",
      "        [-0.0525, -0.0521],\n",
      "        [-0.0515, -0.0480],\n",
      "        [-0.0453, -0.0539],\n",
      "        [-0.0521, -0.0461],\n",
      "        [-0.0406, -0.0597],\n",
      "        [-0.0485, -0.0509],\n",
      "        [-0.0363, -0.0665],\n",
      "        [-0.0435, -0.0573],\n",
      "        [-0.0380, -0.0667],\n",
      "        [-0.0299, -0.0738],\n",
      "        [-0.0593, -0.0400],\n",
      "        [-0.0329, -0.0710],\n",
      "        [-0.0483, -0.0552],\n",
      "        [-0.0308, -0.0700],\n",
      "        [-0.0487, -0.0542],\n",
      "        [-0.0302, -0.0670]], device='cuda:0')\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
      "tensor([[-0.0516, -0.0461],\n",
      "        [-0.0468, -0.0565],\n",
      "        [-0.0467, -0.0549],\n",
      "        [-0.0639, -0.0354],\n",
      "        [-0.0578, -0.0410],\n",
      "        [-0.0413, -0.0589],\n",
      "        [-0.0404, -0.0624],\n",
      "        [-0.0527, -0.0445],\n",
      "        [-0.0421, -0.0587],\n",
      "        [-0.0455, -0.0524],\n",
      "        [-0.0382, -0.0652],\n",
      "        [-0.0528, -0.0464],\n",
      "        [-0.0550, -0.0443],\n",
      "        [-0.0513, -0.0478],\n",
      "        [-0.0476, -0.0519],\n",
      "        [-0.0317, -0.0769],\n",
      "        [-0.0628, -0.0366],\n",
      "        [-0.0450, -0.0569],\n",
      "        [-0.0424, -0.0631],\n",
      "        [-0.0490, -0.0514],\n",
      "        [-0.0510, -0.0486],\n",
      "        [-0.0552, -0.0450],\n",
      "        [-0.0458, -0.0533],\n",
      "        [-0.0505, -0.0501],\n",
      "        [-0.0499, -0.0512],\n",
      "        [-0.0553, -0.0437],\n",
      "        [-0.0456, -0.0582],\n",
      "        [-0.0423, -0.0617],\n",
      "        [-0.0408, -0.0631],\n",
      "        [-0.0529, -0.0460],\n",
      "        [-0.0473, -0.0585],\n",
      "        [-0.0535, -0.0450]], device='cuda:0')\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 0], device='cuda:0')\n",
      "tensor([[-0.0287, -0.0712],\n",
      "        [-0.0322, -0.0730],\n",
      "        [-0.0501, -0.0488],\n",
      "        [-0.0360, -0.0655],\n",
      "        [-0.0522, -0.0486],\n",
      "        [-0.0436, -0.0578],\n",
      "        [-0.0593, -0.0373],\n",
      "        [-0.0424, -0.0592],\n",
      "        [-0.0572, -0.0424],\n",
      "        [-0.0499, -0.0574],\n",
      "        [-0.0327, -0.0687],\n",
      "        [-0.0323, -0.0689],\n",
      "        [-0.0532, -0.0465],\n",
      "        [-0.0436, -0.0605],\n",
      "        [-0.0613, -0.0402],\n",
      "        [-0.0461, -0.0567],\n",
      "        [-0.0512, -0.0484],\n",
      "        [-0.0427, -0.0589],\n",
      "        [-0.0543, -0.0452],\n",
      "        [-0.0523, -0.0475],\n",
      "        [-0.0395, -0.0596],\n",
      "        [-0.0498, -0.0505],\n",
      "        [-0.0484, -0.0516],\n",
      "        [-0.0487, -0.0491],\n",
      "        [-0.0524, -0.0459],\n",
      "        [-0.0519, -0.0482],\n",
      "        [-0.0543, -0.0408],\n",
      "        [-0.0354, -0.0670],\n",
      "        [-0.0503, -0.0487],\n",
      "        [-0.0446, -0.0554],\n",
      "        [-0.0355, -0.0666],\n",
      "        [-0.0580, -0.0408]], device='cuda:0')\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[-0.0395, -0.0663],\n",
      "        [-0.0433, -0.0612],\n",
      "        [-0.0618, -0.0377],\n",
      "        [-0.0530, -0.0497],\n",
      "        [-0.0551, -0.0425],\n",
      "        [-0.0411, -0.0587],\n",
      "        [-0.0458, -0.0520],\n",
      "        [-0.0428, -0.0603],\n",
      "        [-0.0606, -0.0387],\n",
      "        [-0.0415, -0.0609],\n",
      "        [-0.0473, -0.0520],\n",
      "        [-0.0415, -0.0582],\n",
      "        [-0.0605, -0.0378],\n",
      "        [-0.0479, -0.0558],\n",
      "        [-0.0305, -0.0688],\n",
      "        [-0.0601, -0.0368],\n",
      "        [-0.0502, -0.0536],\n",
      "        [-0.0576, -0.0482],\n",
      "        [-0.0517, -0.0428],\n",
      "        [-0.0475, -0.0518],\n",
      "        [-0.0442, -0.0537],\n",
      "        [-0.0498, -0.0524],\n",
      "        [-0.0550, -0.0459],\n",
      "        [-0.0485, -0.0557],\n",
      "        [-0.0660, -0.0306],\n",
      "        [-0.0657, -0.0326],\n",
      "        [-0.0518, -0.0482],\n",
      "        [-0.0391, -0.0648],\n",
      "        [-0.0467, -0.0530],\n",
      "        [-0.0604, -0.0401],\n",
      "        [-0.0597, -0.0399],\n",
      "        [-0.0500, -0.0524]], device='cuda:0')\n",
      "tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "tensor([[-0.0531, -0.0509],\n",
      "        [-0.0662, -0.0336],\n",
      "        [-0.0422, -0.0597],\n",
      "        [-0.0509, -0.0492],\n",
      "        [-0.0364, -0.0667],\n",
      "        [-0.0567, -0.0439],\n",
      "        [-0.0484, -0.0532],\n",
      "        [-0.0413, -0.0605],\n",
      "        [-0.0530, -0.0478],\n",
      "        [-0.0484, -0.0515],\n",
      "        [-0.0297, -0.0767],\n",
      "        [-0.0575, -0.0404],\n",
      "        [-0.0331, -0.0703],\n",
      "        [-0.0521, -0.0467],\n",
      "        [-0.0534, -0.0524],\n",
      "        [-0.0465, -0.0547],\n",
      "        [-0.0481, -0.0543],\n",
      "        [-0.0449, -0.0572],\n",
      "        [-0.0496, -0.0522],\n",
      "        [-0.0479, -0.0550],\n",
      "        [-0.0402, -0.0642],\n",
      "        [-0.0441, -0.0556],\n",
      "        [-0.0484, -0.0557],\n",
      "        [-0.0327, -0.0733],\n",
      "        [-0.0479, -0.0477],\n",
      "        [-0.0355, -0.0641],\n",
      "        [-0.0426, -0.0584],\n",
      "        [-0.0458, -0.0573],\n",
      "        [-0.0357, -0.0676],\n",
      "        [-0.0359, -0.0666],\n",
      "        [-0.0473, -0.0574],\n",
      "        [-0.0417, -0.0619]], device='cuda:0')\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([[-0.0550, -0.0438],\n",
      "        [-0.0441, -0.0587],\n",
      "        [-0.0305, -0.0770],\n",
      "        [-0.0374, -0.0636],\n",
      "        [-0.0525, -0.0454],\n",
      "        [-0.0464, -0.0549],\n",
      "        [-0.0512, -0.0510],\n",
      "        [-0.0364, -0.0652],\n",
      "        [-0.0437, -0.0577],\n",
      "        [-0.0680, -0.0305],\n",
      "        [-0.0425, -0.0612],\n",
      "        [-0.0567, -0.0460],\n",
      "        [-0.0419, -0.0620],\n",
      "        [-0.0302, -0.0687],\n",
      "        [-0.0537, -0.0470],\n",
      "        [-0.0585, -0.0419],\n",
      "        [-0.0485, -0.0514],\n",
      "        [-0.0498, -0.0489],\n",
      "        [-0.0501, -0.0516],\n",
      "        [-0.0500, -0.0519],\n",
      "        [-0.0516, -0.0530],\n",
      "        [-0.0493, -0.0466],\n",
      "        [-0.0326, -0.0707],\n",
      "        [-0.0380, -0.0591],\n",
      "        [-0.0476, -0.0485],\n",
      "        [-0.0513, -0.0473],\n",
      "        [-0.0411, -0.0604],\n",
      "        [-0.0559, -0.0429],\n",
      "        [-0.0520, -0.0437],\n",
      "        [-0.0563, -0.0459],\n",
      "        [-0.0289, -0.0750],\n",
      "        [-0.0503, -0.0518]], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor([[-0.0542, -0.0472],\n",
      "        [-0.0367, -0.0668],\n",
      "        [-0.0477, -0.0558],\n",
      "        [-0.0566, -0.0479],\n",
      "        [-0.0481, -0.0520],\n",
      "        [-0.0502, -0.0514],\n",
      "        [-0.0439, -0.0550],\n",
      "        [-0.0441, -0.0565],\n",
      "        [-0.0413, -0.0623],\n",
      "        [-0.0595, -0.0433],\n",
      "        [-0.0499, -0.0498],\n",
      "        [-0.0442, -0.0512],\n",
      "        [-0.0537, -0.0452],\n",
      "        [-0.0343, -0.0653],\n",
      "        [-0.0571, -0.0433],\n",
      "        [-0.0400, -0.0614],\n",
      "        [-0.0311, -0.0719],\n",
      "        [-0.0426, -0.0574],\n",
      "        [-0.0472, -0.0527],\n",
      "        [-0.0470, -0.0610],\n",
      "        [-0.0554, -0.0405],\n",
      "        [-0.0458, -0.0578],\n",
      "        [-0.0553, -0.0451],\n",
      "        [-0.0403, -0.0598],\n",
      "        [-0.0510, -0.0474],\n",
      "        [-0.0500, -0.0502],\n",
      "        [-0.0320, -0.0688],\n",
      "        [-0.0477, -0.0490],\n",
      "        [-0.0459, -0.0566],\n",
      "        [-0.0492, -0.0514],\n",
      "        [-0.0524, -0.0485],\n",
      "        [-0.0518, -0.0492]], device='cuda:0')\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[-0.0560, -0.0444],\n",
      "        [-0.0470, -0.0492],\n",
      "        [-0.0438, -0.0629],\n",
      "        [-0.0504, -0.0513],\n",
      "        [-0.0439, -0.0580],\n",
      "        [-0.0434, -0.0530],\n",
      "        [-0.0518, -0.0467],\n",
      "        [-0.0456, -0.0518],\n",
      "        [-0.0619, -0.0365],\n",
      "        [-0.0432, -0.0569],\n",
      "        [-0.0339, -0.0707],\n",
      "        [-0.0301, -0.0737],\n",
      "        [-0.0628, -0.0363],\n",
      "        [-0.0471, -0.0550],\n",
      "        [-0.0494, -0.0510],\n",
      "        [-0.0448, -0.0550],\n",
      "        [-0.0387, -0.0607],\n",
      "        [-0.0519, -0.0435],\n",
      "        [-0.0381, -0.0644],\n",
      "        [-0.0404, -0.0663],\n",
      "        [-0.0586, -0.0407],\n",
      "        [-0.0554, -0.0447],\n",
      "        [-0.0463, -0.0559],\n",
      "        [-0.0666, -0.0260],\n",
      "        [-0.0461, -0.0563],\n",
      "        [-0.0491, -0.0518],\n",
      "        [-0.0566, -0.0380],\n",
      "        [-0.0574, -0.0411],\n",
      "        [-0.0328, -0.0718],\n",
      "        [-0.0371, -0.0675],\n",
      "        [-0.0541, -0.0502],\n",
      "        [-0.0547, -0.0438]], device='cuda:0')\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1], device='cuda:0')\n",
      "tensor([[-0.0610, -0.0387],\n",
      "        [-0.0465, -0.0506],\n",
      "        [-0.0510, -0.0491],\n",
      "        [-0.0334, -0.0687],\n",
      "        [-0.0574, -0.0399],\n",
      "        [-0.0593, -0.0401],\n",
      "        [-0.0383, -0.0646],\n",
      "        [-0.0496, -0.0507],\n",
      "        [-0.0562, -0.0455],\n",
      "        [-0.0516, -0.0488],\n",
      "        [-0.0455, -0.0577],\n",
      "        [-0.0455, -0.0557],\n",
      "        [-0.0555, -0.0434],\n",
      "        [-0.0611, -0.0367],\n",
      "        [-0.0335, -0.0669],\n",
      "        [-0.0474, -0.0514],\n",
      "        [-0.0624, -0.0363],\n",
      "        [-0.0454, -0.0574],\n",
      "        [-0.0393, -0.0632],\n",
      "        [-0.0490, -0.0554],\n",
      "        [-0.0325, -0.0690],\n",
      "        [-0.0508, -0.0527],\n",
      "        [-0.0471, -0.0508],\n",
      "        [-0.0435, -0.0603],\n",
      "        [-0.0479, -0.0525],\n",
      "        [-0.0438, -0.0571],\n",
      "        [-0.0604, -0.0336],\n",
      "        [-0.0485, -0.0541],\n",
      "        [-0.0467, -0.0540],\n",
      "        [-0.0534, -0.0435],\n",
      "        [-0.0645, -0.0331],\n",
      "        [-0.0550, -0.0434]], device='cuda:0')\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "tensor([[-0.0357, -0.0698],\n",
      "        [-0.0463, -0.0562],\n",
      "        [-0.0506, -0.0478],\n",
      "        [-0.0386, -0.0625],\n",
      "        [-0.0362, -0.0660],\n",
      "        [-0.0521, -0.0459],\n",
      "        [-0.0528, -0.0509],\n",
      "        [-0.0423, -0.0599],\n",
      "        [-0.0540, -0.0460],\n",
      "        [-0.0511, -0.0549],\n",
      "        [-0.0456, -0.0608],\n",
      "        [-0.0489, -0.0538],\n",
      "        [-0.0653, -0.0330],\n",
      "        [-0.0515, -0.0495],\n",
      "        [-0.0567, -0.0455],\n",
      "        [-0.0486, -0.0496],\n",
      "        [-0.0639, -0.0347],\n",
      "        [-0.0406, -0.0614],\n",
      "        [-0.0350, -0.0687],\n",
      "        [-0.0412, -0.0616],\n",
      "        [-0.0504, -0.0499],\n",
      "        [-0.0432, -0.0575],\n",
      "        [-0.0570, -0.0435],\n",
      "        [-0.0372, -0.0630],\n",
      "        [-0.0517, -0.0504],\n",
      "        [-0.0394, -0.0630],\n",
      "        [-0.0430, -0.0629],\n",
      "        [-0.0414, -0.0542],\n",
      "        [-0.0500, -0.0512],\n",
      "        [-0.0333, -0.0682],\n",
      "        [-0.0460, -0.0579],\n",
      "        [-0.0527, -0.0471]], device='cuda:0')\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "tensor([[-0.0390, -0.0655],\n",
      "        [-0.0513, -0.0508],\n",
      "        [-0.0356, -0.0668],\n",
      "        [-0.0449, -0.0556],\n",
      "        [-0.0313, -0.0728],\n",
      "        [-0.0391, -0.0626],\n",
      "        [-0.0489, -0.0550],\n",
      "        [-0.0505, -0.0475],\n",
      "        [-0.0595, -0.0420],\n",
      "        [-0.0429, -0.0577],\n",
      "        [-0.0456, -0.0557],\n",
      "        [-0.0435, -0.0528],\n",
      "        [-0.0643, -0.0320],\n",
      "        [-0.0496, -0.0514],\n",
      "        [-0.0418, -0.0587],\n",
      "        [-0.0510, -0.0484],\n",
      "        [-0.0452, -0.0534],\n",
      "        [-0.0596, -0.0430],\n",
      "        [-0.0466, -0.0540],\n",
      "        [-0.0489, -0.0524],\n",
      "        [-0.0442, -0.0606],\n",
      "        [-0.0515, -0.0455],\n",
      "        [-0.0424, -0.0540],\n",
      "        [-0.0568, -0.0424],\n",
      "        [-0.0474, -0.0590],\n",
      "        [-0.0370, -0.0613],\n",
      "        [-0.0571, -0.0435],\n",
      "        [-0.0582, -0.0394],\n",
      "        [-0.0384, -0.0616],\n",
      "        [-0.0507, -0.0553],\n",
      "        [-0.0716, -0.0233],\n",
      "        [-0.0392, -0.0569]], device='cuda:0')\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[-0.0612, -0.0418],\n",
      "        [-0.0523, -0.0436],\n",
      "        [-0.0361, -0.0667],\n",
      "        [-0.0438, -0.0555],\n",
      "        [-0.0437, -0.0554],\n",
      "        [-0.0529, -0.0461],\n",
      "        [-0.0434, -0.0585],\n",
      "        [-0.0601, -0.0435],\n",
      "        [-0.0521, -0.0480],\n",
      "        [-0.0474, -0.0571],\n",
      "        [-0.0479, -0.0525],\n",
      "        [-0.0420, -0.0594],\n",
      "        [-0.0455, -0.0581],\n",
      "        [-0.0364, -0.0668],\n",
      "        [-0.0624, -0.0388],\n",
      "        [-0.0388, -0.0667],\n",
      "        [-0.0597, -0.0397],\n",
      "        [-0.0490, -0.0522],\n",
      "        [-0.0475, -0.0541],\n",
      "        [-0.0524, -0.0508],\n",
      "        [-0.0578, -0.0426],\n",
      "        [-0.0354, -0.0679],\n",
      "        [-0.0448, -0.0568],\n",
      "        [-0.0418, -0.0591],\n",
      "        [-0.0417, -0.0561],\n",
      "        [-0.0391, -0.0633],\n",
      "        [-0.0624, -0.0338],\n",
      "        [-0.0417, -0.0617],\n",
      "        [-0.0566, -0.0471],\n",
      "        [-0.0617, -0.0352],\n",
      "        [-0.0526, -0.0458],\n",
      "        [-0.0581, -0.0410]], device='cuda:0')\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "tensor([[-0.0631, -0.0389],\n",
      "        [-0.0407, -0.0629],\n",
      "        [-0.0512, -0.0485],\n",
      "        [-0.0598, -0.0427],\n",
      "        [-0.0450, -0.0558],\n",
      "        [-0.0571, -0.0442],\n",
      "        [-0.0405, -0.0623],\n",
      "        [-0.0442, -0.0590],\n",
      "        [-0.0530, -0.0477],\n",
      "        [-0.0428, -0.0574],\n",
      "        [-0.0467, -0.0547],\n",
      "        [-0.0472, -0.0519],\n",
      "        [-0.0396, -0.0618],\n",
      "        [-0.0569, -0.0426],\n",
      "        [-0.0322, -0.0707],\n",
      "        [-0.0565, -0.0405],\n",
      "        [-0.0394, -0.0613],\n",
      "        [-0.0483, -0.0502],\n",
      "        [-0.0604, -0.0385],\n",
      "        [-0.0576, -0.0452],\n",
      "        [-0.0469, -0.0539],\n",
      "        [-0.0505, -0.0513],\n",
      "        [-0.0394, -0.0622],\n",
      "        [-0.0416, -0.0582],\n",
      "        [-0.0438, -0.0564],\n",
      "        [-0.0363, -0.0643],\n",
      "        [-0.0415, -0.0582],\n",
      "        [-0.0491, -0.0504],\n",
      "        [-0.0472, -0.0542],\n",
      "        [-0.0512, -0.0517],\n",
      "        [-0.0571, -0.0415],\n",
      "        [-0.0520, -0.0448]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[232], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      3\u001b[0m   running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[200], line 21\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 21\u001b[0m   image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ato\\Documents\\Programming\\Python\\catdog\\env\\Lib\\site-packages\\PIL\\Image.py:2200\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2192\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2193\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2194\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2195\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2196\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2197\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2198\u001b[0m         )\n\u001b[1;32m-> 2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  \n",
    "  running_loss = 0.0\n",
    "\n",
    "  for i, data in enumerate(val_loader, 0):\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = cnn(images)\n",
    "\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    if i%20 == 19:\n",
    "      running_loss += loss\n",
    "      print(f'loss: {running_loss / 50:.5f}')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of CNN(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=90768, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=120, bias=True)\n",
       "  (fc3): Linear(in_features=120, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "cnn.load_state_dict(torch.load(model_path))\n",
    "cnn.eval()\n",
    "cnn.to(device)\n",
    "cnn.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL ACCURACY: 0.5564\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "\n",
    "for i, data in enumerate(testing_loader):\n",
    "  imgs, labels = data\n",
    "  imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "  output = cnn(imgs)\n",
    "  probabilities = F.softmax(output, dim=1)\n",
    "  predicted_class = torch.argmax(probabilities, dim=1)\n",
    "  if predicted_class == labels:\n",
    "    acc.append(int(1))\n",
    "  else:\n",
    "    acc.append(int(0))\n",
    "  \n",
    "accuracy = sum(acc)/len(testing_loader)\n",
    "print(f\"TOTAL ACCURACY: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
